---
layout: post
title: "1. 线性模型"
description: "线性模型"
categories: [机器学习]
tags: [机器学习]
comments: true
---
# 目录：

* Kramdown table of contents
{:toc .toc}

线性模型一般用于**回归**和**分类**

# 一、线性回归

![]({{ site.url }}/assets/images/machinelearning/linear_02.jpg){:height="30%" width="30%"}

如图所示，回归主要是完成对数据的拟合。

## （一）回归模型

> 对数据进行线性拟合

这里设定样本参数维度为$$d$$，样本数量维度是$$n$$。因而$$X$$的维度是$$n \times d$$，$$w$$的维度是$$d \times 1$$，$$Y$$的维度是$$n \times 1$$。

1. 针对单个样本

$$ y_i = a(x_i) = w^T x_i = w_0 + w_1 x_{i,1} + \cdots + w_d x_{i,d}  
= \begin{pmatrix}w_0 & w_1 & \cdots & w_d \end{pmatrix} \begin{pmatrix} 1 \\ x_{i,1} \\ \vdots \\ x_{i,d} \end{pmatrix}$$

2. 针对$$n$$个样本

$$ Y = a(X) = Xw 
= \begin{pmatrix} w_0 + w_1 x_{1,1} + \cdots + w_d x_{1,d} \\w_0 + w_1 x_{2,1} + \cdots + w_d x_{2,d} \\ \vdots \\ w_0 + w_1 x_{n,1} + \cdots + w_d x_{n,d}\end{pmatrix}
= \begin{pmatrix}1 & x_{11} & \cdots & x_{1d} \\ 1 & x_{21} & \cdots & x_{2d} \\ \vdots \\ 1 & x_{n1} & \cdots & x_{nd} \\ \end{pmatrix} 
$$

## （二）损失函数

> 对线性拟合效果进行评估

换句话说，如何去选择最优的拟合直线。

线性回归中最常使用的是**最小二乘法（OLS）**，也称为平方损失函数。

1. 最小二乘法 

$$
\begin{eqnarray}
L(w) & = & \frac{1}{2n} \sum_{i=1}^n[a(x_i)-y_i]^2  \\
& = & \frac{1}{2n} \sum_{i=1}^n[w_0 + w_1 x_{i,1} + \cdots + w_d x_{i,d}-y_i]^2\\
& = & \frac{1}{2n} \sum_{i=1}^n[w^T x_i-y_i]^2\\
& = & \frac{1}{2n} \begin{pmatrix}w^T x_1-y_1 & \cdots & w^T x_n-y_n \end{pmatrix} \begin{pmatrix}w^T x_1-y_1 \\ \cdots \\ w^T x_n-y_n \end{pmatrix} \\
& = & \frac{1}{2n} \begin{bmatrix}Xw-Y \end{bmatrix}^T \begin{bmatrix} Xw-Y \end{bmatrix}\\
& = & \frac{1}{2n} \begin{Vmatrix}Xw-Y \end{Vmatrix}^2\\
\end{eqnarray}
$$

之所以写成$$\frac{1}{2n}$$，而不是$$\frac{1}{n}$$，是为了方便求导时，可以与2相消。

2. 代码
~~~ python 
# 这里假设X,Y,w都是np.mat格式
def loss(X, Y, w):
	 n = Y.shape[0]
	 return 1.0/(2*n) * (X*w - Y).T * (X*w - Y)
~~~


## （三）梯度下降

> 求解参数w需要最小化损失函数

针对线性回归，最简单粗暴求解方法是，直接求导，令导数等于0。

$$\frac{\partial L(w)}{\partial w} = \frac{1}{n} X^T (Xw-Y) = 0$$

$$
\begin{eqnarray}
X^T (Xw-Y) & = & 0  \\
X^T X w &= & X^T Y\\
w &= & (X^T X)^{-1} X^T Y
\end{eqnarray}
$$

直接针对矩阵进行运算，相对于梯度下降法无需多次迭代，但是当数据量很大，即$$X$$过大时，矩阵运算需要很大的计算复杂度。

如果矩阵X是奇异矩阵，求逆则会非常不稳定。因此人们常用梯度下降法作为优化方法，通过不断迭代来得到最小化损失函数的最优解。

1. 梯度下降

第一步：初始化 $$w^0$$

第二步：计算梯度 $$\nabla L(w) = \begin{pmatrix} \frac{\partial L(w)}{\partial w_0} & \cdots & \frac{\partial L(w)}{\partial w_d} \end{pmatrix} ^T$$

第三步：更新参数 $$w^{t} = w^{t-1} - \nabla L(w^{t-1}) \cdot \eta $$

到达终值条件以后，更新停止，最终的$$w$$为线性回归模型的最终参数。

$$\eta$$是学习率。

以本节为例，得到 ：

$$\nabla L(w) = \begin{pmatrix} \frac{\partial L(w)}{\partial w_0} & \cdots & \frac{\partial L(w)}{\partial w_d} \end{pmatrix} ^T

= \frac{1}{n} X^T (Xw-Y)$$

非矩阵的写法是：

$$
\begin{eqnarray}
\frac{\partial L(w)}{\partial w_j} 
& = & \frac{1}{2n} \frac{\partial \sum_{i=1}^n[w_0 + w_1 x_{i,1} + \cdots + w_d x_{i,d}-y_i]^2}{{\partial w_j}}\\ 
& = & \frac{1}{n} \sum_{i=1}^{n}(w^T x_i-y_i)x_{i,j}
\end{eqnarray}
$$

2. 代码




参考文献：
