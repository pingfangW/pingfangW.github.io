---
layout: post
title: "4. 卷积神经网络"
description: "卷积神经网络"
categories: [机器学习]
tags: [机器学习]
comments: true
---
# 目录：

* Kramdown table of contents
{:toc .toc}

卷积网络：

专门用来处理具有类似网格结构的数据

卷积：对两个实变函数的数学运算

s(t) = (x*w)(t)

= \inft (x(a)w(t-a)da

输入：x
核函数：w
特征映射：s(t) 也叫输出

离散时：

s(t) = \sum x(a)w(t-a)

二维的公式：

S(i, j) = (K*I)( i, j ) = \sum_m \sum_n I(m, n) K( i - m, j - n )

卷积是可交换的：

S(i, j) =(K*I)( i, j ) = \sum_m \sum_n I(i-m, j-n)K(m, n)

卷积运算可交换性是因为将核相对输入进行了反转。

可交换性在证明时很有用，但是在应用中不是一个重要的性质。

互相关函数(cross-correlation)：

S(i , j) = (I * K)(i , j) = \sum_m \sum_n I( i+m, j+n)K(m , n)

这是很多神经网络库会实现的一个函数，和卷积运算几乎一样，但是并没有对核进行翻转。

这里把这两种都叫做卷积。

一个基于核翻转的卷积运算的学习算法，所学得的核，是对未进行翻转的算法学得的核的翻转。

卷积通常是与其他函数一起使用，无论卷积运算是否对核进行了翻转，这些函数的组合通常是不可交换的。

卷积运算：
稀疏交互：使核的大小远小于输入的大小。在深度卷积网络中，处在网络深层的单元可能与绝大部分输入是间接交互的。
参数共享：在一个模型的多个函数中使用相同的参数。核的每一个元素都作用在输入的每一位置上。保证了只需要学习一个参数集合，而不是对每一个位置都需要学习一个单独的参数集合。使得神经网络层具有对平移等变的性质。
等变表示：卷积对缩放或旋转等变换并不是天然等变的。


卷积网络：一个典型层包括三级：
第一级：并行的计算多个卷积产生一组线性激活响应

第二级：每个线性激活响应将通过一个非线性的激活函数（探测级）

第三级：使用池化函数来进一步调整这一层的输出

池化函数使用某一位置的相邻输出的总体捅进特征来替代网络在该位置的输出。

不管采用什么样的池化函数，当输入作少量平移时，池化能够帮助输入的表示近似不变。

使用池化可以看作是增加了一个无限强的先验。

这一层学得的函数必须就有对少量平移的不变性。

当这个假设成立时，池化可以极大地提高网络的统计效率。

也就是对空间区域进行池化产生了平移不变性。

池化对处理不同大小的输入具有重要作用

将特征一起动态地池化也是可行的



可以把卷积网络看做全连接网络。

只是对网络的权重施加了一个无限强的先验：

一个隐藏单元的权重必须和它邻居的权重相同，但可以在空间移动；

除了接受域的权重外，其他权重都为零。

总结：在网络中的一层的参数引入一个无限强的先验概率分布。

这个先验说明了该吃呢个应该学得的函数只包含局部连接关系并且对平移具有等变性。


池化，也是无限强的先验：每个单元都具有对少量平移的不变性。

卷积和池化可能导致欠拟合。

与其他先验类似，卷积和池化只有当先验的假设合理且正确时才有用。

如果一项任务依赖于保存精确的空间信息，那么在所有的特征上使用池化将会增大训练误差 

当一项任务涉及到要对输入中相隔较远的信息进行合并时，那么卷积所利用的先验可能就不正确了。 



比较卷积模型的统计学习表现时，只能以基准中的其 他卷积模型作为比较的对象 



基本卷积函数的变体：

神经网络中的卷积，通常是指多个并行卷积组成的运算。
因为具有单个核的卷积只能提取一种类型的特征。
通常希望网络的每一层能够在多个位置提取多种类型的特征。

通常在每个位置包含多个不同卷积的输出。


因为卷积网络通常使用多通道的卷积，所以即使使用了核翻转，也不一定保证网络的线性运算是可交换的。

只有当其中的每个运算的输出和输入具有相同的通道数时，这些多通道的运算才是可交换的。


下采样

三种零填充设定


不使用零填充（有效卷积）：
输出像素的表示更规范，但限制了层数的增加

使用足够的零填充（相同卷积）：
输入输出保持相同大小，但会带来边界像素的欠表示

足够多的零填充（全卷积）：
每个像素在每个方向恰好被访问了k次，最终输出图像的宽度为m+k=1
输出像素中靠近边界的部分相比于中间部分是更少像素的函数

通常零填充的最优数量（对测试集的分类正确率）处于有效卷积和相同卷积之间的某个位置。

在一些情况下，我们并不是真的想使用卷积，而是想用一些局部连接的网络层。

在这种情况下， 多层感知机对应的邻接矩阵是相同的， 但每一个连接都有它自己的权重

也被称为非共享卷积：和具有一个小核的离散卷积运算很像，但并不横跨位置来共享参数。

当我们知道每一个特征都是一小块空间的函数，并且相同的特征不会出现在所有的空间上时，局部连接层是很有用的。

平铺卷积（tiled convolution）：对卷积层和局部连接层进行了折中。
并不是对每一个空间位置的权重集合进行学习。

学习一组核在空间移动时可以循环利用。意味着在近邻的位置上拥有不同的过滤器，就像局部连接层一样。

对参数的存储需求仅仅会增长常数倍。这个常数是核的集合的大小，而不是整个输出的特征映射的大小。

局部连接层与平铺卷积层都和最大池化有一些关联：这些层的探测单元 都是由不同的过滤器驱动的。如果这些过滤器能够学会探测相同隐含特征的不同变换形式，那么最大池化的单元对于学得的变换就具有不变性 


卷积是一种线性运算，可以表示成矩阵乘法的形式。其中包含的矩阵是关于卷积核的函数。这个 矩阵是稀疏的并且核的每个元素都复制给矩阵的多个元素。 这种观点能够帮助我们 导出实现一个卷积网络所需的很多其他运算。 


卷积神经网络可以用于输出高维的结构化对象，而不仅仅是预测分类任务的类 标签或回归任务的实数值。 


经常出现的一个问题是输出平面可能比输入平面要小 

用于 对图像中单个对象分类的常用结构中，网络空间维数的最大减少来源于使用大步 幅的池化层。为了产生与输入大小相似的输出映射，我们可以避免把池化放在一起 

或单纯地产生一张低分辨率的标签网格 ；原则上可以使用具有单位步幅的池化操作。 

数据：卷积网络使用的数据通常包含多个通道，每个通道是时间上或空间中某一点的 
不同观测量。 

卷积网络的一个优点是，它们还可以处理具有可变的空间尺度的输入 


利用并行计算资源的强大实现是很关键的 

卷积等效于使用傅立叶变换将输入与核都转换到频域、执行两个信号的逐点相乘，再使用傅立叶逆变换转换回时域。对于某些问题的规模，这种算法可能比离散 卷积的朴素实现更快 


卷积网络训练中最昂贵的部分是学习特征 

有三种基本策略可以不通过监督训练而得到卷积核：随机、手动、无监督

随机过滤器经常在卷积网络中表现得出乎意料得好



# 一、




参考文献：

1. deep learning
