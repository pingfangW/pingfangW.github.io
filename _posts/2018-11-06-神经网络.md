---
layout: post
title: "神经网络"
description: "神经网络"
categories: [神经网络]
tags: [神经网络]
redirect_from:
  - /2018/10/23/
---

# 目录

* Kramdown table of contents
{:toc .toc}

# 正文

## 一、单一神经元网络

![]({{ site.url }}/assets/images/networks/onenetwork01.jpg)

一个神经元网络是最简单最基础的神经网络。

如上图所示：

输入是 $$x_1, x_2, x_3 和 截距 +1 $$

输出是 $$h_{x,b}(x)$$

中间的运算过程是 $$ f(W_1x_1 + W_2x_2 + W_3x_3)=f(W^Tx)=h_{W,b}(x) $$

这里的f(·)被称为激活函数。

这里激活函数选择sigmoid函数，公式如下：
$$ f(z) = \frac{1}{1+e^{-z}} $$


## 二、多个神经元网络

![]({{ site.url }}/assets/images/networks/network01.jpg)

神经网络就是将许多单一“神经元”联结在一起。

其中：

$$L_1$$是输入层，$$L_2$$是隐藏层，$$L_3$$是输出层。

参数：$$(W, b) = (W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)})$$

$$ W^{(l)}_{ij} $$ 是 第l层第j单元 与 第 l+1层第i单元 之间的联结参数。
$$ b^{(l)}_{i} $$ 是 第l+1层第i单元的偏置项。

$$ a^{(l)}_{i} $$ 是 第l层第i大暖的激活值（输出值），那么：

$$ a^(1)_1 = x_1, a^(1)_2 = x_2, a^(1)_3 = x_3$$

$$ a^{(2)}_1 = f(W^{(1)}_{11}x_1 + W^{(1)}_{12}x_2 + W^{(1)}_{13}x_3 + b^{(1)}_1) $$
$$ a^{(2)}_2 = f(W^{(1)}_{21}x_1 + W^{(1)}_{22}x_2 + W^{(1)}_{23}x_3 + b^{(2)}_1) $$
$$ a^{(2)}_3 = f(W^{(1)}_{31}x_1 + W^{(1)}_{22}x_2 + W^{(1)}_{33}x_3 + b^{(3)}_1) $$

$$ h_{W,b}(x)= a^{(3)}_1 = f(W^{(2)}_{11}a^{(2)}_1 + W^{(2)}_{12}a^{(2)}_2 + W^{(2)}_{13}a^{(2)}_1 + b^{(2)}_1) $$

重新撰写是：

$$ z^{(l+1)} = W^{(l)}a^{(l)} + b^{(l)} $$
$$ a^{(l+1)} = f(z^{(l+1)}) $$

先计算第一层，再计算第二层，再计算第三层，依次计算，直到输出层。

这里依次向前计算，没有闭环或回路，被称为**前馈神经网络**。


## 三、多个输出单元

![]({{ site.url }}/assets/images/networks/network02.jpg)


## 四、自编码神经网络

前面的神经网络都是监督学习，即训练样本是有类别标签的

**自编码神经网络**是无监督学习算法，其训练样本是不带类别标签的。训练样本集合为 $$\{x^{(1)}, x^{(2)}, x^{(3)}, \ldots\}$$ ，其中 $$x^{(i)} \in \Re^{n}$$

自编码神经网络使用反向传播算法，让目标值等于输入值， 比如$$y^{(i)} = x^{(i)}$$ 。

下图是一个自编码神经网络的示例。

![]({{ site.url }}/assets/images/networks/network03.jpg)

自编码神经网络尝试学习一个 $$h_{W,b}(x) \approx x$$ 的函数。

换句话说，它尝试逼近一个恒等函数，从而使得输出 $$\hat{x}$$ 接近于输入 $$x$$ 。

恒等函数虽然看上去不太有学习的意义，但是当我们为自编码神经网络加入某些限制，

比如限定隐藏神经元的数量，我们就可以从输入数据中发现一些有趣的结构。

举例来说，假设某个自编码神经网络的输入 $$x$$ 是一张 $$10 \times 10$$ 图像（共100个像素）的像素灰度值，

于是 $$n=100$$ ，其隐藏层 $$L_2$$ 中有50个隐藏神经元。注意，输出也是100维的 $$y \in \Re^{100}$$ 。

由于只有50个隐藏神经元，我们迫使自编码神经网络去学习输入数据的压缩表示，

也就是说，它必须从50维的隐藏神经元激活度向量 $$a^{(2)} \in \Re^{50}$$ 中重构出100维的像素灰度值输入$$x$$ 。

如果网络的输入数据是完全随机的，比如每一个输入 $$x_i$$ 都是一个跟其它特征完全无关的独立同分布高斯随机变量，那么这一压缩表示将会非常难学习。

但是如果输入数据中隐含着一些特定的结构，比如某些输入特征是彼此相关的，那么这一算法就可以发现输入数据中的这些相关性。

事实上，这一简单的自编码神经网络通常可以学习出一个跟主元分析（PCA）结果非常相似的输入数据的低维表示。


我们刚才的论述是基于隐藏神经元数量较小的假设。

但是即使隐藏神经元的数量较大（可能比输入像素的个数还要多），我们仍然通过给自编码神经网络施加一些其他的限制条件来发现输入数据中的结构。

具体来说，如果我们给隐藏神经元加入稀疏性限制，那么自编码神经网络即使在隐藏神经元数量较多的情况下仍然可以发现输入数据中一些有趣的结构。

稀疏性可以被简单地解释如下。

如果当神经元的输出接近于1的时候我们认为它被激活，而输出接近于0的时候认为它被抑制，那么使得神经元大部分的时间都是被抑制的限制则被称作稀疏性限制。这里我们假设的神经元的激活函数是sigmoid函数。如果你使用tanh作为激活函数的话，当神经元输出为-1的时候，我们认为神经元是被抑制的。

注意到 $$a^{(2)}_j$$ 表示隐藏神经元 $$j$$ 的激活度，

但是这一表示方法中并未明确指出哪一个输入 $$x$$ 带来了这一激活度。

所以我们将使用 $$a^{(2)}_j(x)$$ 来表示在给定输入为 $$x$$ 情况下，自编码神经网络隐藏神经元 $$j$$ 的激活度。 

进一步，让

$$
\begin{align}
\hat\rho_j = \frac{1}{m} \sum_{i=1}^m \left[ a^{(2)}_j(x^{(i)}) \right]
\end{align}
$$

表示隐藏神经元 $$j$$ 的平均活跃度（在训练集上取平均）。

我们可以近似的加入一条限制

$$
\begin{align}
\hat\rho_j = \rho,
\end{align}
$$

其中， $$\rho$$ 是稀疏性参数，通常是一个接近于0的较小的值（比如$$\rho$$ = 0.05 ）。

换句话说，我们想要让隐藏神经元 $$j$$ 的平均活跃度接近0.05。

为了满足这一条件，隐藏神经元的活跃度必须接近于0。

为了实现这一限制，我们将会在我们的优化目标函数中加入一个额外的惩罚因子，而这一惩罚因子将惩罚那些 $$\hat\rho_j$$ 和 $$\rho$$ 有显著不同的情况从而使得隐藏神经元的平均活跃度保持在较小范围内。

?????? 为什么要使得活跃度比较低呢

惩罚因子的具体形式有很多种合理的选择，我们将会选择以下这一种：

$$
\begin{align}
\sum_{j=1}^{s_2} \rho \log \frac{\rho}{\hat\rho_j} + (1-\rho) \log \frac{1-\rho}{1-\hat\rho_j}.
\end{align}
$$

这里， $$s_2$$ 是隐藏层中隐藏神经元的数量，而索引$$j$$依次代表隐藏层中的每一个神经元。

如果你对相对熵（KL divergence）比较熟悉，这一惩罚因子实际上是基于它的。

于是惩罚因子也可以被表示为

$$
\begin{align}
\sum_{j=1}^{s_2} {\rm KL}(\rho || \hat\rho_j),
\end{align}
$$


其中 \textstyle {\rm KL}(\rho || \hat\rho_j)
 = \rho \log \frac{\rho}{\hat\rho_j} + (1-\rho) \log \frac{1-\rho}{1-\hat\rho_j} 是一个以 \textstyle \rho 为均值和一个以 \textstyle \hat\rho_j 为均值的两个伯努利随机变量之间的相对熵。相对熵是一种标准的用来测量两个分布之间差异的方法。（如果你没有见过相对熵，不用担心，所有你需要知道的内容都会被包含在这份笔记之中。）


参考文献：

http://ufldl.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C