---
layout: post
title: "神经网络"
description: "神经网络"
categories: [神经网络]
tags: [神经网络]
redirect_from:
  - /2018/10/23/
---

# 目录

* Kramdown table of contents
{:toc .toc}

神经网络向量化：

http://deeplearning.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%90%91%E9%87%8F%E5%8C%96

向量化练习：http://deeplearning.stanford.edu/wiki/index.php/Exercise:Vectorization

# 正文

## 一、单一神经元网络

![]({{ site.url }}/assets/images/networks/onenetwork01.png)

一个神经元网络是最简单最基础的神经网络。

如上图所示：

输入是 $$x_1, x_2, x_3 和 截距 +1 $$

输出是 $$h_{x,b}(x)$$

中间的运算过程是 $$ f(W_1x_1 + W_2x_2 + W_3x_3)=f(W^Tx)=h_{W,b}(x) $$

这里的f(·)被称为激活函数。

这里激活函数选择sigmoid函数，公式如下：
$$ f(z) = \frac{1}{1+e^{-z}} $$


## 二、多个神经元网络

![]({{ site.url }}/assets/images/networks/network01.png)

神经网络就是将许多单一“神经元”联结在一起。

其中：

$$L_1$$是输入层，$$L_2$$是隐藏层，$$L_3$$是输出层。

参数：$$(W, b) = (W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)})$$

$$ W^{(l)}_{ij} $$ 是 第l层第j单元 与 第 l+1层第i单元 之间的联结参数。
$$ b^{(l)}_{i} $$ 是 第l+1层第i单元的偏置项。

$$ a^{(l)}_{i} $$ 是 第l层第i大暖的激活值（输出值），那么：

$$ a^(1)_1 = x_1, a^(1)_2 = x_2, a^(1)_3 = x_3$$

$$ a^{(2)}_1 = f(W^{(1)}_{11}x_1 + W^{(1)}_{12}x_2 + W^{(1)}_{13}x_3 + b^{(1)}_1) $$
$$ a^{(2)}_2 = f(W^{(1)}_{21}x_1 + W^{(1)}_{22}x_2 + W^{(1)}_{23}x_3 + b^{(2)}_1) $$
$$ a^{(2)}_3 = f(W^{(1)}_{31}x_1 + W^{(1)}_{22}x_2 + W^{(1)}_{33}x_3 + b^{(3)}_1) $$

$$ h_{W,b}(x)= a^{(3)}_1 = f(W^{(2)}_{11}a^{(2)}_1 + W^{(2)}_{12}a^{(2)}_2 + W^{(2)}_{13}a^{(2)}_1 + b^{(2)}_1) $$

重新撰写是：

$$ z^{(l+1)} = W^{(l)}a^{(l)} + b^{(l)} $$
$$ a^{(l+1)} = f(z^{(l+1)}) $$

先计算第一层，再计算第二层，再计算第三层，依次计算，直到输出层。

这里依次向前计算，没有闭环或回路，被称为**前馈神经网络**。


## 三、多个输出单元

![]({{ site.url }}/assets/images/networks/network02.png)


## 四、自编码神经网络（稀疏）

符号：

|符号|	含义|
|-------|-------|
|$$x$$	|训练样本的输入特征，$$x \in \Re^{n}$$.|
|$$y$$	|输出值/目标值. 这里 $$y$$ 可以是向量. 在autoencoder中，$$y=x$$.|
|$$(x^{(i)}, y^{(i)})$$	|第$$i$$个训练样本|
|$$h_{W,b}(x)$$	|输入为$$x$$时的假设输出，其中包含参数$$W,b$$. 该输出应当与目标值$$y$$具有相同的维数.|
|$$W^{(l)}_{ij}$$	|连接第$$l$$层$$j$$单元和第$$l+1$$层$$i$$单元的参数.|
|$$b^{(l)}_{i}$$| 第$$l+1$$层$$i$$单元的偏置项. 也可以看作是连接第$$l$$层偏置单元和第$$l+1$$层$$i$$单元的参数.|
|$$\theta$$ | 参数向量. 可以认为该向量是通过将参数$$W,b$$ 组合展开为一个长的列向量而得到.|
|$$a^{(l)}_i$$ |网络中第$$l$$层$$i$$单元的激活（输出）值.另外，由于$$L_1$$层是输入层，所以$$a^{(1)}_i = x_i$$.||
|$$f(\cdot)$$	|激活函数. 本文中我们使用$$f(z) = \tanh(z)$$.|
|$$z^{(l)}_i$$	|第$$l$$层$$i$$单元所有输入的加权和. 因此有$$a^{(l)}_i = f(z^{(l)}_i)$$.|
|$$\alpha$$	|学习率|
|$$s_l$$	| 第$$l$$层的单元数目（不包含偏置单元）.|
|$$n_l$$ 	|网络中的层数. 通常$$L_1$$层是输入层，$$L_{n_l}$$ 层是输出层.|
|$$\lambda$$ 	|权重衰减系数.|
|$$\hat{x}$$	| 对于一个autoencoder，该符号表示其输出值；亦即输入值$$x$$的重构值. 与$$h_{W,b}(x)$$含义相同.|
|$$\rho$$	|稀疏值，可以用它指定我们所需的稀疏程度|
|$$\hat\rho_i$$	|（sparse autoencoder中）隐藏单元$$i$$的平均激活值.|
|$$\beta$$	|（sparse autoencoder目标函数中）稀疏值惩罚项的权重.|

练习：http://deeplearning.stanford.edu/wiki/index.php/Exercise:Sparse_Autoencoder

### 1、自编码神经网络介绍

前面的神经网络都是监督学习，即训练样本是有类别标签的

**自编码神经网络**是无监督学习算法，其训练样本是不带类别标签的。训练样本集合为 $$\{x^{(1)}, x^{(2)}, x^{(3)}, \ldots\}$$ ，其中 $$x^{(i)} \in \Re^{n}$$

自编码神经网络使用反向传播算法，让目标值等于输入值， 比如$$y^{(i)} = x^{(i)}$$ 。

下图是一个自编码神经网络的示例。

![]({{ site.url }}/assets/images/networks/network03.png)

自编码神经网络尝试学习一个 $$h_{W,b}(x) \approx x$$ 的函数。

换句话说，它尝试逼近一个恒等函数，从而使得输出 $$\hat{x}$$ 接近于输入 $$x$$ 。

恒等函数虽然看上去不太有学习的意义，但是当我们为自编码神经网络加入某些限制，

比如限定隐藏神经元的数量，我们就可以从输入数据中发现一些有趣的结构。

举例来说，假设某个自编码神经网络的输入 $$x$$ 是一张 $$10 \times 10$$ 图像（共100个像素）的像素灰度值，

于是 $$n=100$$ ，其隐藏层 $$L_2$$ 中有50个隐藏神经元。注意，输出也是100维的 $$y \in \Re^{100}$$ 。

由于只有50个隐藏神经元，我们迫使自编码神经网络去学习输入数据的压缩表示，

也就是说，它必须从50维的隐藏神经元激活度向量 $$a^{(2)} \in \Re^{50}$$ 中重构出100维的像素灰度值输入$$x$$ 。

如果网络的输入数据是完全随机的，比如每一个输入 $$x_i$$ 都是一个跟其它特征完全无关的独立同分布高斯随机变量，那么这一压缩表示将会非常难学习。

但是如果输入数据中隐含着一些特定的结构，比如某些输入特征是彼此相关的，那么这一算法就可以发现输入数据中的这些相关性。

事实上，这一简单的自编码神经网络通常可以学习出一个跟主元分析（PCA）结果非常相似的输入数据的低维表示。


我们刚才的论述是基于隐藏神经元数量较小的假设。

但是即使隐藏神经元的数量较大（可能比输入像素的个数还要多），我们仍然通过给自编码神经网络施加一些其他的限制条件来发现输入数据中的结构。

具体来说，如果我们给隐藏神经元加入稀疏性限制，那么自编码神经网络即使在隐藏神经元数量较多的情况下仍然可以发现输入数据中一些有趣的结构。

稀疏性可以被简单地解释如下。

如果当神经元的输出接近于1的时候我们认为它被激活，而输出接近于0的时候认为它被抑制，那么使得神经元大部分的时间都是被抑制的限制则被称作稀疏性限制。这里我们假设的神经元的激活函数是sigmoid函数。如果你使用tanh作为激活函数的话，当神经元输出为-1的时候，我们认为神经元是被抑制的。

注意到 $$a^{(2)}_j$$ 表示隐藏神经元 $$j$$ 的激活度，

但是这一表示方法中并未明确指出哪一个输入 $$x$$ 带来了这一激活度。

所以我们将使用 $$a^{(2)}_j(x)$$ 来表示在给定输入为 $$x$$ 情况下，自编码神经网络隐藏神经元 $$j$$ 的激活度。 

进一步，让

$$
\begin{align}
\hat\rho_j = \frac{1}{m} \sum_{i=1}^m \left[ a^{(2)}_j(x^{(i)}) \right]
\end{align}
$$

表示隐藏神经元 $$j$$ 的平均活跃度（在训练集上取平均）。

我们可以近似的加入一条限制

$$
\begin{align}
\hat\rho_j = \rho,
\end{align}
$$

其中， $$\rho$$ 是稀疏性参数，通常是一个接近于0的较小的值（比如$$\rho$$ = 0.05 ）。

换句话说，我们想要让隐藏神经元 $$j$$ 的平均活跃度接近0.05。

为了满足这一条件，隐藏神经元的活跃度必须接近于0。

为了实现这一限制，我们将会在我们的优化目标函数中加入一个额外的惩罚因子，而这一惩罚因子将惩罚那些 $$\hat\rho_j$$ 和 $$\rho$$ 有显著不同的情况从而使得隐藏神经元的平均活跃度保持在较小范围内。

?????? 为什么要使得活跃度比较低呢

惩罚因子的具体形式有很多种合理的选择，我们将会选择以下这一种：

$$
\begin{align}
\sum_{j=1}^{s_2} \rho \log \frac{\rho}{\hat\rho_j} + (1-\rho) \log \frac{1-\rho}{1-\hat\rho_j}.
\end{align}
$$

这里， $$s_2$$ 是隐藏层中隐藏神经元的数量，而索引$$j$$依次代表隐藏层中的每一个神经元。

如果你对相对熵（KL divergence）比较熟悉，这一惩罚因子实际上是基于它的。

于是惩罚因子也可以被表示为

$$
\begin{align}
\sum_{j=1}^{s_2} {\rm KL}(\rho || \hat\rho_j),
\end{align}
$$


其中 

$${\rm KL}(\rho || \hat\rho_j)
 = \rho \log \frac{\rho}{\hat\rho_j} + (1-\rho) \log \frac{1-\rho}{1-\hat\rho_j} $$
 
是一个以 $$\rho$$ 为均值和一个以 $$\hat\rho_j$$ 为均值的两个伯努利随机变量之间的相对熵。

相对熵是一种标准的用来测量两个分布之间差异的方法。

这一惩罚因子有如下性质，当 $$\hat\rho_j = \rho$$ 时 $${\rm KL}(\rho || \hat\rho_j) = 0$$ ，

并且随着 $$\hat\rho_j$$ 与 $$\rho$$ 之间的差异增大而单调递增。

举例来说，在下图中，我们设定 $$\rho = 0.2$$ 

并且画出了相对熵值 $${\rm KL}(\rho || \hat\rho_j)$$ 随着 $$\hat\rho_j$$ 变化的变化。

![]({{ site.url }}/assets/images/networks/network04.png)

我们可以看出，相对熵在 $$\hat\rho_j = \rho$$ 时达到它的最小值0，而当 $$\hat\rho_j$$ 靠近0或者1的时候，相对熵则变得非常大（其实是趋向于$$\infty$$）。

所以，最小化这一惩罚因子具有使得 $$\hat\rho_j$$ 靠近 $$\rho$$ 的效果。 现在，我们的总体代价函数可以表示为

$$
\begin{align}
J_{\rm sparse}(W,b) = J(W,b) + \beta \sum_{j=1}^{s_2} {\rm KL}(\rho || \hat\rho_j),
\end{align}
$$

其中 $$J(W,b)$$ 如之前所定义，而 $$\beta$$ 控制稀疏性惩罚因子的权重。 

$$\hat\rho_j$$ 项则也（间接地）取决于 $$W,b$$ ，因为它是隐藏神经元 $$j$$ 的平均激活度，而隐藏层神经元的激活度取决于 $$W,b$$ 。


为了对相对熵进行导数计算，我们可以使用一个易于实现的技巧，这只需要在你的程序中稍作改动即可。

具体来说，前面在后向传播算法中计算第二层（ $$l=2$$ ）更新的时候我们已经计算了

$$
\begin{align}
\delta^{(2)}_i = \left( \sum_{j=1}^{s_{2}} W^{(2)}_{ji} \delta^{(3)}_j \right) f'(z^{(2)}_i),
\end{align}
$$

现在我们将其换成

$$
\begin{align}
\delta^{(2)}_i =
  \left( \left( \sum_{j=1}^{s_{2}} W^{(2)}_{ji} \delta^{(3)}_j \right)
+ \beta \left( - \frac{\rho}{\hat\rho_i} + \frac{1-\rho}{1-\hat\rho_i} \right) \right) f'(z^{(2)}_i) .
\end{align}
$$

就可以了。


有一个需要注意的地方就是我们需要知道 $$\hat\rho_i$$ 来计算这一项更新。

所以在计算任何神经元的后向传播之前，你需要对所有的训练样本计算一遍前向传播，从而获取平均激活度。

如果你的训练样本可以小到被整个存到内存之中（对于编程作业来说，通常如此），你可以方便地在你所有的样本上计算前向传播并将得到的激活度存入内存并且计算平均激活度 。

然后你就可以使用事先计算好的激活度来对所有的训练样本进行后向传播的计算。

如果你的数据量太大，无法全部存入内存，你就可以扫过你的训练样本并计算一次前向传播，然后将获得的结果累积起来并计算平均激活度 $$\hat\rho_i$$ 

（当某一个前向传播的结果中的激活度 $$a^{(2)}_i$$被用于计算平均激活度 $$\hat\rho_i$$ 之后就可以将此结果删除）。

然后当你完成平均激活度 $$\hat\rho_i$$ 的计算之后，你需要重新对每一个训练样本做一次前向传播从而可以对其进行后向传播的计算。

对于后一种情况，你对每一个训练样本需要计算两次前向传播，所以在计算上的效率会稍低一些。

如果想要使用经过以上修改的后向传播来实现自编码神经网络，

那么你就会对目标函数 $$J_{\rm sparse}(W,b)$$ 做梯度下降。

使用梯度验证方法，你可以自己来验证梯度下降算法是否正确。。



### 2、自编码器训练结果可视化

训练完（稀疏）自编码器，我们还想把这自编码器学到的函数可视化出来，好弄明白它到底学到了什么。

我们以在10×10图像（即n=100）上训练自编码器为例。在该自编码器中，每个隐藏单元i对如下关于输入的函数进行计算：

$$
\begin{align}
a^{(2)}_i = f\left(\sum_{j=1}^{100} W^{(1)}_{ij} x_j  + b^{(1)}_i \right).
\end{align}
$$

我们将要可视化的函数，就是上面这个以2D图像为输入、并由隐藏单元$$i$$计算出来的函数。

它是依赖于参数$$W^{(1)}_{ij}$$的（暂时忽略偏置项$$b_i$$）。

需要注意的是，$$a^{(2)}_i$$可看作输入$$x$$的非线性特征。

不过还有个问题：什么样的输入图像$$x$$可让$$a^{(2)}_i$$得到最大程度的激励？

（通俗一点说，隐藏单元$$i$$要找个什么样的特征？）。

这里我们必须给$$x$$加约束，否则会得到平凡解。

若假设输入有范数约束$$||x||^2 = \sum_{i=1}^{100} x_i^2 \leq 1$$，

则可证令隐藏单元$$i$$得到最大激励的输入应由下面公式计算的像素$$x_j$$给出（共需计算100个像素，j=1,…,100）：

$$
\begin{align}
x_j = \frac{W^{(1)}_{ij}}{\sqrt{\sum_{j=1}^{100} (W^{(1)}_{ij})^2}}.
\end{align}
$$

当我们用上式算出各像素的值、把它们组成一幅图像、并将图像呈现在我们面前之时，

隐藏单元$$i$$所追寻特征的真正含义也渐渐明朗起来。

假如我们训练的自编码器有100个隐藏单元，可视化结果就会包含100幅这样的图像——每个隐藏单元都对应一幅图像。

审视这100幅图像，我们可以试着体会这些隐藏单元学出来的整体效果是什么样的。


当我们对稀疏自编码器（100个隐藏单元，在10X10像素的输入上训练 ）进行上述可视化处理之后，结果如下所示：

![]({{ site.url }}/assets/images/networks/network04.png)

上图的每个小方块都给出了一个（带有有界范数 的）输入图像$$x$$，它可使这100个隐藏单元中的某一个获得最大激励。

我们可以看到，不同的隐藏单元学会了在图像的不同位置和方向进行边缘检测。

显而易见，这些特征对物体识别等计算机视觉任务是十分有用的。

若将其用于其他输入域（如音频），该算法也可学到对这些输入域有用的表示或特征。


## 何时使用神经网络


1.1：神经网络需要大量的信息数据来训练。将神经网络想象成一个孩子。它首先观察父母如何走路。然后它才会独立行走，并且每走一步，孩子都会学习如何执行特定的任务。如果你不让它走，它可能永远不会学习如何走路。你可以提供给孩子的“数据”越多，效果就越好。

1.2：当你有适当类型的神经网络来解决问题时。 每个问题都有自己的难点。数据决定了你解决问题的方式。例如，如果问题是序列生成，递归神经网络更适合，而如果它是一个图像相关的问题，你可能会采取卷积神经网络。

1.3：硬件要求对于运行深度神经网络模型是至关重要的。神经网络很早以前就被“发现”了，但是近年来，神经网络一直在发光，这是因为计算能力的强大。如果你想用这些网络解决现实生活中的问题，准备购买一些高性能硬件吧！


## 如何解决神经网络问题

神经网络是一种特殊类型的机器学习（ML）算法。因此，与每个ML算法一样，它遵循数据预处理，模型构建和模型评估等常规ML工作流程。我列出了一个如何处理神经网络问题的待办事项清单。

1．检查神经网络是否可以提升传统算法。

2．做一个调查，哪个神经网络架构最适合即将解决的问题。

3．通过你选择的语言/库来定义神经网络架构。

4．将数据转换为正确的格式，并将其分成批。

5．根据你的需要预处理数据。

6．增加数据以增加规模并制作更好的训练模型。

7．将数据批次送入神经网络。

8．训练和监测训练集和验证数据集的变化。

9．测试你的模型，并保存以备将来使用。


最流行的深度学习库是Python提供的API，其次是Lua中，Java和Matlab的。最流行的库是：

Caffe（http://t.cn/8FnOmHE）

DeepLearning4j（http://t.cn/RXidtGH）

TensorFlow（http://t.cn/RYRN172）

Theano（http://t.cn/RGDldaz）

Torch（http://torch.ch/）


## TensorFlow实现神经网络

注意：我们可以使用不同的神经网络体系结构来解决这个问题，但是为了简单起见，我们需要实现前馈多层感知器。

神经网络的常见的实现如下：

1.定义要编译的神经网络体系结构。

2.将数据传输到你的模型。

3.将数据首先分成批次，然后进行预处理。

4.然后将其加入神经网络进行训练。

5.显示特定的时间步数的准确度。

6.训练结束后保存模型以供将来使用。

7.在新数据上测试模型并检查其执行情况。

~~~ python 
%pylab inline

import os 
import numpy as np 
import pandas as pd
from scipy.misc import imread
from sklearn.metrics import accuracy_score
import tensorflow as tf 

# 设置初始值，以便可以控制模型的随机性
seed = 128
rng = np.random.RandomState(seed)

# 第一步：设置保管目录路径
root_dir = os.path.abspath('../..')
data_dir = os.path.join(root_dir, 'data')
sub_dir = os.path.join(root_dir, 'sub')
# 检查目录存在性
os.path.exists(root_dir)
os.path.exists(data_dir)
os.path.exists(sub_dir)

# 读数据
train = pd.read_csv(os.path.join(data_dir, 'Train', 'train.csv'))
test = pd.read_csv(os.path.join(data_dir, 'Test.csv'))
sample_submission = pd.read_csv(os.path.join(data_dir, 'Sample_Submission.csv'))

# 看数据
img_name = rng.choice(train,filename)
filepath = os.path.join(date_dir, 'Train', 'Images', 'train', img_name)
img = imread(filepath, flatten=True)
pylab.imshow(img, cmap='gray')
pylab.axis('off')
pylab.show()

# 为了方便后续数据处理，将所有图像存储为numpy数组
temp = []
for img_name in train.filename:
	image_path = os.path.join(data_dir, 'Train', 'Images', 'train', img_name)
	img = imread(image_path, flatten=True)
	img = img.astype('float32')
	temp.append(img)
train_x = np.stack(temp)

temp = []
for img_name in test.filename:
	image_path = os.path.join(data_dir, 'Train', 'Images', 'test', img_name)
	img = imread(image_path, flatten=True)
	img = img.astype('float32')
	temp.append(img)
test_x = np.stack(temp)

# 为了测试模型的正确性，创建验证集
split_size = int(train_x.shape[0]*0.7)
train_x, val_x = train_x[:split_size], train_x[split_size:]
train_y, val_y = train.label.values[:split_size], train.label.values[split_size:]

# 定义一些辅助函数
def dense_to_one_hot(labels_dense, num_classes=10):
	"""将分类标签转换成one-hot向量"""
	num_labels = labels_dense.shape[0]
	index_offset = np.arange(num_labels)*num_classes
	labels_one_hot = np.zeros((num_labels, num_classes))
	labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1
	return labels_one_hot
	89
def preproc(unclean_batch_x):
	"""将数值范围转换为0-1"""
	temp_batch = unclean_batch_x/unclean_batch_x.max()
	return temp_batch

def batch_creator(batch_size, dataset_length, dataset_name):
	"""将样本随机分块， 并返回适合的格式"""
	batch_mask = rng.choice(dataset_length, batch_size)
	batch_x = eval(dataset_name+'_x')[[batch_mask]].reshape(-1, input_num_units)
	batch_x = preproc(batch_x)
	if dataset_name == 'train':
		batch_y = eval(dataset_name).ix[batch_mask, 'label'].values
		batch_y = dense_to_one_hot(batch_y)
	return batch_x, batch_y
	
# 接下来搭建神经网络

# 设置所有的变量

# 每一层神经元的个数
 input_num_units = 28*28
 hidden_num_units = 500
 output_num_units = 10
 
# 定义变量
x = tf.placeholder(tf.float32, [None, input_num_units])
y = tf.placeholder(tf.float32, [NOne, output_num_units])

# 其他变量
epochs = 5
batch_size = 128
learning_rate 0.01

# 定义权重和偏差
weights = {
	'hidden':tf.Variable(tf.random_normal([input_num_units, hidden_num_units], seed=seed)),
	'output':tf.Variable(tf.random_normal([hidden_num_units, output_num_units], seed=seed))
}

biases = {
	'hidden':tf.Variable(tf.random_normal([hidden_num_units], seed=seed)),
	'output':tf.Variable(tf.random_normal([output_num_units], seed=seed))
}

# 计算神经网络计算图
hidden_layer = tf.add(tf.matmul(x, weights['hidden']), biases['hidden'])
hidden_layer = tf.nn.relu(hidden_layer)

output_layer = tf.matmul(hidden_layer, weights['output']) + biases['output']

cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_layer, y))

optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

init = tf.initialize_all_variables()

# 建会话，运行计算图
with tf.Session() as sess:
	sess.run(init)
	
	for epoch in range(epoches):
		avg_cost = 0
		total_batch = int(train.shape[0]/batch_size)
		for i in range(total_batch):
			batch_x, batch_y = batch_creator(batch_size, train_x.shape[0], 'train')
			_, c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})
			
			avg_cost = c/total_batch
		
		print("Epoch:", (epoch+1), "cost =", "{:,5f}".format(avg_cost))
		
	print("\nTraining complete")
	
	# 验证集
	pred_temp = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y, 1))
	accuracy = tf.reduce_mean(tf.cost(pred_temp, "float"))
	print("Validation Accuracy:", accuracy.eval({x: val_x.reshape(-1, input_num_units), y:dense_to_one_hot(val_y)}))
	
	predict = tf.argmax(output_layer, 1)
	pred = predict.eval({x: test_x.reshape(-1, input_num_units)})
~~~

## RBF网络

RBF网络能够逼近任意非线性的函数。可以处理系统内难以解析的规律性，具有很好的泛化能力，并且具有较快的学习速度。

当网络的一个或多个可调参数（权值或阈值）对任何一个输出都有影响时，这样的网络称为**全局逼近网络**。


由于对于每次输入，网络上的每一个权值都要调整，从而导致全局逼近网络的学习速度很慢，比如BP网络。

如果对于输入空间的某个局部区域只有少数几个连接权值影响输出，则该网络称为**局部逼近网络**，比如RBF网络。

接下来重点先介绍RBF网络的原理，然后给出其实现。先看如下图

![]({{ site.url }}/assets/images/networks/network05.jpg)

RBF首先要选择P个基函数，

参考文献：

http://ufldl.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C

https://mp.weixin.qq.com/s/lrWH4yWglIr161Y5bmu5OA

https://mp.weixin.qq.com/s/SrGfpcgqxtPQUD-Y5rFhEw