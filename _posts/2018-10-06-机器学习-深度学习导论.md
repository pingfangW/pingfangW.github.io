---
layout: post
title: "机器学习系列课程-深度学习导论"
description: "深度学习"
categories: [机器学习]
tags: [机器学习, 深度学习]
redirect_from:
  - /2018/10/23/
---

# 目录

* Kramdown table of contents
{:toc .toc}

# 正文

Advanced machine learning specialization 机器学习专业化进阶

## 一、线性模型

线性模型一般用于**回归**和**分类**

### 1.1 回归

![]({{ site.url }}/assets/images/machinelearning/linear_02.jpg){:height="50%" width="50%"}

如图所示，回归主要是完成对数据的拟合。

![]({{ site.url }}/assets/images/machinelearning/linear_03.jpg)

用代码实现为：

~~~
 
~~~

#################################################################################

用于回归的线性模型公式：

a(x) = b + w_1 x_1 + w_2 x_2 + ... + w_d x_d = w^T


判断回归模型拟合效果好坏的指标是均方误差（MSE，Mean Squared error)

L(w) = 1/L*sum((w^T*x_i - y_i)^2) = 1/L*||Xw - y||^2

直接的解是： (X^T*X)^{-1}*X^T*Y

用梯度下降法求解是：

1. 初始化   ： w_0

2. 计算梯度 ：L'(w_0) = 

#################################################################################

![]({{ site.url }}/assets/images/machinelearning/linear_01.jpg)

如上图所示，用直线进行分类。

x,y

回归和分类。均方误差，损失函数

在分类中的应用，y=-1,1

y = 1,2,...,k 多类

判断效果好坏：accuracy loss, Iverson bracket。其实就是看预测结果正确所占的比重。

但是这个指标有两个缺点：在优化过程中一般使用梯度去计算损失函数，而这个损失函数没有梯度，其次是无法计算置信度。

可以用损失函数替代 

a(x) = sign(w^Tx)

交叉熵（cross entropy）

二元/多元

梯度：

终止条件，既可以判断wt-1和wt之间的差是不是足够小，也可以判断损失函数的值的变化是不是变小，还有就是看梯度向量的模是不是接近于0.

有很多问题值得探讨，比如怎么去初始化w0，如何选择步幅eta_t，什么时候停止，如何去估计梯度。

什么情况下，线性模型的analytical solution和MSE loss有效：

训练集和测试集。

交叉训练（cross-validation)训练多次，但像深度神经网络在多个GPU上仍可能训练很多周，训练很多次是不太可行的，一般就选一个测试集。在数据量比较大的情况下，还是具有代表性的。

使用惩罚项的原因是：

在没有过度拟合的情况下，系数的数值一般会小一些。

因此把这个特点看做是特征，认为过度拟合的模型会有较大的权重，而好的模型没有较大的权重。

因此为了解决过度拟合问题，对大的权重进行惩罚。

这样总觉得不对，会不会导致前后结果差别很大？ 比如可能8次方的系数原先很大，然后惩罚以后8次方就被毙了，这种情况怎么说

这里主要是集中在在众多的特征中选择重要的特征。要和另外一种情况做区分：就是所有的特征都有用，想做的是合成少量的新特征。

随机梯度下降（stochastic gradient descent）可以用在在线学习上（online learning），而且步幅（学习率）对随机梯度下降法影响更大，需要谨慎选择。

如果在一个非常大的样本上训练模型，内存不够存，怎么做。使用随机梯度下降，把样本存在硬盘上，每次读一个例子。

为了克服随机梯度下降的缺点，可以使用mini-batch gradient descent（选择m个随机样本）

## 一、深度学习介绍（deep learning)