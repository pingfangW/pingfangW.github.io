---
layout: post
title: "2. BP神经网络"
description: "BP神经网络"
categories: [机器学习]
tags: [机器学习]
comments: true
---
# 目录：

* Kramdown table of contents
{:toc .toc}

# 一、单一神经元网络

![]({{ site.url }}/assets/ML/OLS_BP_network_01.jpg){:height="40%" width="40%"}

单一神经元网络是最简单最基础的神经网络，这里第一层叫做输入层，第二层是隐藏层，第三层是输出层

上面这个图用公式表示出来就是：

$$Y = a(Z) = a(Xw) = a(w_0 + w_1 x_1 + \cdots + w_d x_d)$$

在神经网络中，$$a(·)$$一般称为激活函数。

在单一神经元网络里，选择线性的激活函数就相当于我们前文中的线性回归，选择sigmoid激活函数，就相当于前文中的逻辑回归。

我们以神经网络的形式重新编写线性回归和逻辑回归的代码，以获取对神经网络的初步认识：

## （一）线性回归-BP神经网络

（1）激活函数

很显然，线性回归里$$a(X)=X$$，所以：

~~~ python 
def ols_activation(Z):
	return Z
~~~

（2）向前传播

向前传播就是从输入X到输出Y的计算过程

~~~ python 
def ols_forward(X, w):
	Z = X*w
	a = ols_activation(Z)
	return a
~~~

其实这里输出就是X*w，之所以多加一个activation也是为了统一整个计算过程。

得到输出值以后，要计算输出值和真实值之间的差距，从而得到使差距最小的参数，在线性模型当中，我们使用梯度下降法，这里也是梯度下降法，只是后续会涉及到多层网络，计算过程更为复杂，统一称为**反向传播**，也就是BP传播

（3）反向传播（BP）

因为这里只有一层网络，所以和线性模型的梯度下降基本上一致，我们采取神经网络的写法：

同样是先计算梯度值：

~~~ python 
def OLS_dev(X, Y, w):
	return 1.0/n*X.T*(X*w-Y)
~~~

计算反向传播（梯度下降）过程：

~~~ python 
def OLS_BP(X, Y, w, lr, epoches):
	for i in range(epoches):
		w -= lr * OLS_dev(X, Y, w)
	return w
~~~

为了便于理解，这里梯度下降写的比前面简易一些

可以看到这里名称也发生了一些变换，之前的num_iters换成了epoches，eta换成了lr（learning rate）

（4）整合

整合起来就是：

~~~ python
class OLS_single_network():
    def __init__(self, epoches, lr):
        self.epoches = epoches
        self.lr = lr
        
    def ols_activation(self, X):
        return X
    
    
    def ols_forward(self, X, w):
        Z = X*w
        a = self.ols_activation(Z)
        return a
    
    def ols_dev(self, X, Y, w, input_num):
        return 1.0/input_num*X.T*(X*w-Y)
    
    def ols_BP(self, X, Y):
        input_num = X.shape[0]
        hidden_num = X.shape[1]
        output_num = 1
        w = np.mat(np.random.random((hidden_num, output_num)))
        for i in range(self.epoches):
            w -= self.lr * self.ols_dev(X, Y, w, input_num)
        
        return w
            
    def fit(self, X, Y):
        w = self.ols_BP(X, Y)
        y_predict = self.ols_forward(X, w)
        input_num = X.shape[0]
        print("train accuracy:{}%".format(100-1./input_num*np.sqrt(np.sum(np.square(y_predict-Y)))*100))
        return y_predict
~~~

运行
~~~ python
from sklearn.datasets import make_regression
import numpy as np
import matplotlib.pyplot as plt

X, Y = make_regression(n_samples=1000, n_features=1, noise=5) 
print(X.shape, Y.shape)
# ((1000, 1), (1000,))

# 这里没有专门区分w_i和w_0，因为没有正则化
X = np.insert(X, 0, values=1, axis=1)
X = np.mat(X)
Y = np.mat(Y).T
print(X.shape, Y.shape)
# ((1000, 2), (1000, 1))

lr = OLS_single_network(epoches=1000, lr=0.1)
y_predict = lr.fit(X, Y)
# train accuracy:83.6044609236%

xx = X[:,1].tolist()
yy = Y.tolist()
plt.scatter(xx, yy)
plt.plot(xx, y_predict.tolist(), c='r')
plt.show()
~~~

![]({{ site.url }}/assets/ML/OLS_BP_network_02.jpg){:height="40%" width="40%"}

## （一）逻辑回归-BP神经网络

（1）激活函数

这里的激活函数就是sigmoid函数，即

~~~ python
def LR_activation(Z):
	return 1./(1.+np.exp(-Z))
~~~

（2）向前传播

~~~ python
def LR_forward(X, w):
	Z = X*w
	a = LR_activation(Z)
	return a
~~~

（3）反向传播（BP）

~~~ python
def LR_dev(X, Y, w):
	y_predict = LR_forward(X, w)
	return -(1./n)*X.T*(Y - Y_predict)
~~~

~~~ python
def LR_BP(X, Y, w, lr, epoches):
	for i in range(epoches):
		w -= lr * LR_dev(X, Y, w)
	return w
~~~

（4）整合

~~~ python
 class LR_single_network():
    def __init__(self, epoches, lr):
        self.epoches = epoches
        self.lr = lr
        
    def lr_activation(self, Z):
        return 1./(1.+np.exp(-Z))
    
    
    def lr_forward(self, X, w):
        Z = X*w
        a = self.lr_activation(Z)
        return a
    
    def lr_dev(self, X, Y, w, input_num):
        y_predict = self.lr_forward(X, w)
        return -(1./input_num)*X.T*(Y - y_predict)
    
    def lr_BP(self, X, Y):
        input_num = X.shape[0]
        hidden_num = X.shape[1]
        output_num = 1
        w = np.mat(np.random.random((hidden_num, output_num)))
        for i in range(self.epoches):
            w -= self.lr * self.lr_dev(X, Y, w, input_num)
        return w
            
    def fit(self, X, Y):
        w = self.lr_BP(X, Y)
        y_predict = self.lr_forward(X, w)
        y_predict[y_predict > 0.5] = 1
        y_predict[y_predict <= 0.5] = 0
        input_num = X.shape[0]
        print("train accuracy:{}%".format(np.sum(y_predict==Y)/float(input_num) * 100))
        return y_predict
~~~

实现：

~~~ python 
from sklearn.datasets import make_classification
import numpy as np
import matplotlib.pyplot as plt

X, Y = make_classification(n_samples=1000, n_features=20,  n_classes=2) 
print(X.shape, Y.shape)

X = np.insert(X, 0, values=1, axis=1)
X = np.mat(X)
Y = np.mat(Y).T
print(X.shape, Y.shape)

lr = LR_single_network(epoches=1000, lr=0.1)
y_predict = lr.fit(X, Y)

# ((1000, 20), (1000,))
# ((1000, 21), (1000, 1))
# train accuracy:92.7%
~~~

# 二、多元神经元网络

如果我们上面的神经网络再多一层呢？

## （一）两层神经网络

![]({{ site.url }}/assets/ML/two_layer_netwok_01.jpg){:height="40%" width="40%"}

这是一个两层的神经网络，第一层是输入层，第二层和第三层是隐藏层，第四层是输出层

如果第二层的激活函数不变，对应到前面的线性回归和逻辑回归就是在原先的基础上多增加了一次计算。

### （1）线性回归-BP神经网络

第一层到第二层的计算：

$$
\begin{eqnarray}
Z^1_i & = & w^1_{i0} + w^1_{i1} x_1 + \cdots + w^1_{id} x_{d} \\
& = &  \begin{pmatrix} w^1_{i0} & w^1_{i1} & \cdots & w^1_{id} \end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ \vdots & x_{d} \end{pmatrix}
\end{eqnarray}
$$

维度是（1×n），写成矩阵形式是：

$$
\begin{eqnarray}
Z^1 & = & w^1_{0} + w^1_{1} x_1 + \cdots + w^1_{d} x_{d} \\
& = &  \begin{pmatrix} w^1_{0} & w^1_{1} & \cdots & w^1_{d} \end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ \vdots & x_{d} \end{pmatrix}
\end{eqnarray}
$$

维度是$$（d_1×n）$$

$$ a^1_i = Z^1_i $$

$$ a^1 = Z^1 $$

第二层到第三层的计算：

$$
\begin{eqnarray}
Z^2_1 & = & w^2_0 + w^2_1 a^1_1 + \cdots + w^2_{d_1} a^1_{d_1} \\
& = &  \begin{pmatrix} w^2_{0} & w^2_{1} & \cdots & w^2_{d_1} \end{pmatrix} \begin{pmatrix} 1 \\ a^1_1 \\ \vdots & a^1_{d_1} \end{pmatrix}
\end{eqnarray}
$$

维度是（1×n）

$$ a^2_1 = Z^2_1 $$

第三层到输出层

$$ Y_{predict} = a^2_1 $$

（1.1）激活函数

与前面相同

（1.2）向前传播
~~~ python
def ols_forward(X, w1, w2):
	Z1 = X * w1
	a1 = ols_activation(Z1)
	Z2 = a1 * w2
	a2 = ols_activation(Z2)
	result = {'a1':a1,
			  'a2':a2}
	return a2
~~~

（1.3）反向传播（BP）

层数增加最大的区别就在BP部分，求梯度需要用到链式法则，但是原理一样，都是损失函数对需要更新的参数求导：

$$
\begin{eqnarray}
L(w) & = & \frac{1}{2n} \sum_{i=1}^n[a^2_1-y_i]^2  \\
& = & \frac{1}{2n} \sum_{i=1}^n[w^2_0 + w^2_1 a^1_1 + \cdots + w^2_{d_1} a^1_{d_1}-y_i]^2\\
\end{eqnarray}
$$

对$$w^2_k$$求梯度：

$$
\begin{eqnarray}
\frac{\partial L(w)}{\partial w^2_k}
& = &  \frac{\partial L(w)}{\partial a^2_1}\frac{\partial a^2_1}{\partial w^2_k} \\
& = & \frac{1}{n} \sum_{i=1}^{n}(a^2_1-y_i)a^1_k
\end{eqnarray}
$$

对$$w^{ij}$$求梯度

$$
\begin{eqnarray}
\frac{\partial L(w)}{\partial w^1_{ij}}
& = &  \frac{\partial L(w)}{\partial a^2_1}\frac{\partial a^2_1}{\partial a^1_i}\frac{\partial a^1_i}{\partial w^1_{ij}} \\
& = & \frac{1}{n} \sum_{i=1}^{n}(a^2_1-y_i)w^2_i x_j
\end{eqnarray}
$$

~~~ python 
def ols_dev(X, Y, w1, w2):
	result = ols_forward(X, w1, w2)
	a1 = result['a1']
	a2 = result['a2']
	w2_dev = 1./n * (a2 - Y) * a1 
	w1_dev = 1./n * (a2 - Y) * w2 * X 
	w_dev = {'w1_dev': w1_dev,
			 'w2_dev': w2_dev}
	return w_dev
~~~

~~~ python 
def ols_BP(X, Y, w1, w2, epoches, lr):
	for i in range(epoches):
		w_dev = lr_dev(X, Y, w1, w2)
		w1_dev = w_dev['w1_dev']
		w2_dev = w_dev['w2_dev']
		w1 -= lr*w1_dev
		w2 -= lr*w2_dev
	
	w = {'w1':w1,
		 'w2':w2}
	return w
~~~

但是看两层参数的梯度值可以发现，w2的梯度值和a1的数值相关，a1又和w1相关，而w1的梯度值和w2相关。

因此更理想的做法是，在一次迭代的时候的更新w2的数值，然后用更新完的w2数值，去计算w1的梯度，然后再用w1计算新的a1，再用来计算w2的梯度。因此更新上面的代码如下：

~~~ python 
def ols_dev_w2(X, Y, a1, w1, a2, w2):
	w2_dev = 1./n * (a2 - Y) * a1 
	return w2_dev 
	
def ols_dev_w1(X, Y, a2, w2):
	w1_dev = 1./n * (a2 - Y) * w2 * X
	return w1_dev 

def ols_BP(X, Y, w1, w2, epoches, lr):
	for i in range(epoches):
	
		result = ols_forward(X, w1, w2)
		a1 = result['a1']
		a2 = result['a2']		
		w2_dev = lr_dev_w2(X, Y, a1, w1, a2, w2)
		w2 -= lr*w2_dev
		
		result = ols_forward(X, w1, w2)
		a2 = result['a2']				
		w1_dev = lr_dev_w1(X, Y, a2, w2)
		w1 -= lr*w1_dev
		
	w = {'w1':w1,
		 'w2':w2}
	return w	
~~~

（1.4）整合

~~~ python 
class OLS_two_network():
    def __init__(self, epoches, lr, hidden2_num):
        self.lr = lr
        self.epoches = epoches
        self.hidden2_num = hidden2_num
        
    def ols_activation(self, Z):
        return Z
    
    def ols_forward(self, X, w1, w2):
        Z1 = X * w1
        a1 = self.ols_activation(Z1)
        Z2 = a1 * w2
        a2 = self.ols_activation(Z2)
        result = {'a1':a1,
                  'a2':a2}
        return result
    
    def ols_dev_w2(self, X, Y, a1, w1, a2, w2, input_num):
        w2_dev = 1./input_num * a1.T * (a2 - Y)
        return w2_dev
    
    def ols_dev_w1(self, X, Y, a2, w2, input_num):
        w1_dev = 1./input_num * X.T * (a2 - Y) * w2.T
        return w1_dev
    
    def ols_BP(self, X, Y):
        input_num = X.shape[0]
        hidden_num = X.shape[1]
        output_num = 1
        w1 = np.mat(np.random.random((hidden_num, self.hidden2_num)))
        w2 = np.mat(np.random.random((self.hidden2_num, output_num)))
        for i in range(self.epoches):
            result = self.ols_forward(X, w1, w2)
            a1 = result['a1']
            a2 = result['a2']
            w2_dev = self.ols_dev_w2(X, Y, a1, w1, a2, w2, input_num)
            w2 -= self.lr*w2_dev
            
            result = self.ols_forward(X, w1, w2)
            a2 = result['a2']
            w1_dev = self.ols_dev_w1(X, Y, a2, w2, input_num)
            w1 -= self.lr*w1_dev
        
        w = {'w1':w1,
             'w2':w2}
        return w
    
    def fit(self, X, Y):
        w = self.ols_BP(X, Y)
        w1 = w['w1']
        w2 = w['w2']
        result = self.ols_forward(X, w1, w2)
        y_predict = result['a2']
        input_num = X.shape[0]
        print("train accuracy:{}%".format(100-1./input_num*np.sqrt(np.sum(np.square(y_predict-Y)))*100))
        return y_predict
~~~

实现：

~~~ python
from sklearn.datasets import make_regression
import numpy as np
import matplotlib.pyplot as plt

X, Y = make_regression(n_samples=1000, n_features=10, noise=5) 
print(X.shape, Y.shape)

X = np.insert(X, 0, values=1, axis=1)
X = np.mat(X)
Y = np.mat(Y).T
print(X.shape, Y.shape)

lr = OLS_two_network(epoches=1000, lr=0.001, hidden2_num=5)
y_predict = lr.fit(X, Y)
# ((1000, 10), (1000,))
# ((1000, 11), (1000, 1))
# train accuracy:84.4941378548%
~~~

### （2）逻辑回归-BP神经网络

第一层到第二层的计算：

$$
\begin{eqnarray}
Z^1_i & = & w^1_{i0} + w^1_{i1} x_1 + \cdots + w^1_{id} x_{d} \\
& = &  \begin{pmatrix} w^1_{i0} & w^1_{i1} & \cdots & w^1_{id} \end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ \vdots & x_{d} \end{pmatrix}
\end{eqnarray}
$$

维度是（1×n），写成矩阵形式是：

$$
\begin{eqnarray}
Z^1 & = & w^1_{0} + w^1_{1} x_1 + \cdots + w^1_{d} x_{d} \\
& = &  \begin{pmatrix} w^1_{0} & w^1_{1} & \cdots & w^1_{d} \end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ \vdots & x_{d} \end{pmatrix}
\end{eqnarray}
$$

维度是$$（d_1×n）$$

$$ a^1_i =  = \frac{1}{1+e^{- Z^1_i}}$$

$$ a^1 =  = \frac{1}{1+e^{- Z^1}}$$

第二层到第三层的计算：

$$
\begin{eqnarray}
Z^2_1 & = & w^2_0 + w^2_1 a^1_1 + \cdots + w^2_{d_1} a^1_{d_1} \\
& = &  \begin{pmatrix} w^2_{0} & w^2_{1} & \cdots & w^2_{d_1} \end{pmatrix} \begin{pmatrix} 1 \\ a^1_1 \\ \vdots & a^1_{d_1} \end{pmatrix}
\end{eqnarray}
$$

维度是（1×n）

$$ a^2_1 =  = \frac{1}{1+e^{- Z^2_1}}$$

第三层到输出层

$$ Y_{predict} = a^2_1 $$

（1.1）激活函数


与前面相同

（1.2）向前传播

~~~ python
def lr_forward():
~~~


	
反向传播算法：

　　三层神经网络：Layer L1是输入层，Layer L2是隐含层，Layer L3是隐含层，

手里有一堆数据{x1,x2,x3,...,xn},输出也是一堆数据{y1,y2,y3,...,yn},

现在要他们在隐含层做某种变换，让你把数据灌进去后得到你期望的输出。

如果你希望你的输出和原始输入一样，那么就是最常见的自编码模型（Auto-Encoder）。

如果你的输出和原始输入不一样，那么就是很常见的人工神经网络了，

相当于让原始数据通过一个映射来得到我们想要的输出数据，也就是我们今天要讲的话题。








参考文献：

1. deep learning


~~~ python
# -*- coding: utf-8 -*-
'''
Created on

@author: Belle
'''
from numpy.random.mtrand import randint
import numpy as np


'''双曲函数'''
def tanh(value):
    return (1 / (1 + np.math.e ** (-value)))

'''双曲函数的导数'''
def tanhDer(value):
    tanhValue = tanh(value)
    return tanhValue * (1 - tanhValue)

'''
Bp神经网络model
'''
class BpNeuralNetWorkModel:
    def __init__(self, trainningSet, label, layerOfNumber, studyRate):
        '''学习率'''
        self.studyRate = studyRate
        '''计算隐藏层神经元的数量'''
        self.hiddenNeuronNum = int(np.sqrt(trainningSet.shape[1] + label.shape[1]) + randint(1, 10))
        '''层数据'''
        self.layers = []
        '''创建输出层'''
        currentLayer = Layer()
        currentLayer.initW(trainningSet.shape[1], self.hiddenNeuronNum)
        self.layers.append(currentLayer)
        
        '''创建隐藏层'''
        for index in range(layerOfNumber - 1):
            currentLayer = Layer()
            self.layers.append(currentLayer)
            '''输出层后面不需要求权重值'''
            if index == layerOfNumber - 2:
                break
            nextLayNum = 0
            
            '''初始化各个层的权重置'''
            if index == layerOfNumber - 3:
                '''隐藏层到输出层'''
                nextLayNum = label.shape[1]
            else:
                '''隐藏层到隐藏层'''
                nextLayNum = self.hiddenNeuronNum
            currentLayer.initW(self.hiddenNeuronNum, nextLayNum)
        '''输出层的分类值'''
        currentLayer = self.layers[len(self.layers) - 1]
        currentLayer.label = label
    
    '''神经网络前向传播'''
    def forward(self, trainningSet):
        '''计算输入层的输出值'''
        currentLayer = self.layers[0]
        currentLayer.alphas = trainningSet
        currentLayer.caculateOutPutValues()
        
        preLayer = currentLayer
        for index in range(1, len(self.layers)):
            currentLayer = self.layers[index]
            '''上一层的out put values就是这一层的zValues'''
            currentLayer.zValues = preLayer.outPutValues
            '''计算alphas'''
            currentLayer.caculateAlphas()
            '''最后一层不需要求输出值，只要求出alpha'''
            if index == len(self.layers) - 1:
                break
            '''输入层计算out puts'''
            currentLayer.caculateOutPutValues()
            '''指向上一层的layer'''
            preLayer = currentLayer
    
    '''神经网络后向传播'''
    def backPropogation(self):
        layerCount = len(self.layers)
        
        '''输出层的残差值'''
        currentLayer = self.layers[layerCount - 1]
        currentLayer.caculateOutPutLayerError()
        
        '''输出层到隐藏层'''
        preLayer = currentLayer
        layerCount = layerCount - 1
        while layerCount >= 1:
            '''当前层'''
            currentLayer = self.layers[layerCount - 1]
            '''更新权重'''
            currentLayer.updateWeight(preLayer.errors, self.studyRate)
            if layerCount != 1:
                currentLayer.culateLayerError(preLayer.errors)
            layerCount = layerCount - 1
            preLayer = currentLayer
            
'''
创建层
'''
class Layer:
    def __init__(self):
        self.b = 0
    
    '''使用正态分布的随机值初始化w的值'''
    def initW(self, numOfAlpha, nextLayNumOfAlpha):
        self.w = np.mat(np.random.randn(nextLayNumOfAlpha, numOfAlpha))
    
    '''计算当前层的alphas'''
    def caculateAlphas(self):
        '''alpha = f(z)'''
        self.alphas = np.mat([tanh(self.zValues[row1,0]) for row1 in range(len(self.zValues))])
        '''求f'(z)的值（即f的导数值）'''
        self.zDerValues = np.mat([tanhDer(self.zValues[row1,0]) for row1 in range(len(self.zValues))])
    
    '''计算out puts'''
    def caculateOutPutValues(self):
        '''计算当前层z = w * alpha的的下一层的输入值'''
        self.outPutValues = self.w * self.alphas.T + self.b
    
    '''计算输出层的残差'''
    def caculateOutPutLayerError(self):
        self.errors = np.multiply(-(self.label - self.alphas), self.zDerValues)
        print("out put layer alphas ..." + str(self.alphas))
    
    '''计算其它层的残差'''
    def culateLayerError(self, preErrors):
        self.errors = np.mat([(self.w[:,column].T * preErrors.T * self.zDerValues[:,column])[0,0] for column in range(self.w.shape[1])])
    
    '''更新权重'''
    def updateWeight(self, preErrors, studyRate):
        data = np.zeros((preErrors.shape[1], self.alphas.shape[1]))
        for index in range(preErrors.shape[1]):
            data[index,:] = self.alphas * (preErrors[:,index][0,0])
        self.w = self.w - studyRate * data

'''
训练神经网络模型
@param train_set: 训练样本
@param labelOfNumbers: 训练总类别
@param layerOfNumber:  神经网络层数，包括输出层，隐藏层和输出层(默认只有一个输入层，隐藏层和输出层)
'''
def train(train_set, label, layerOfNumber = 3, sampleTrainningTime = 5000, studyRate = 0.6):
    neuralNetWork = BpNeuralNetWorkModel(train_set, label, layerOfNumber, studyRate)
    '''训练数据'''
    for row in range(train_set.shape[0]):
        '''当个样本使用梯度下降的方法训练sampleTrainningTime次'''
        for time in range(sampleTrainningTime):
            '''前向传播 '''
            neuralNetWork.forward(train_set[row,:])
            '''反向传播'''
            neuralNetWork.backPropogation()
            




~~~
