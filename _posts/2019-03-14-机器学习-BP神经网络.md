---
layout: post
title: "2. BP神经网络"
description: "BP神经网络"
categories: [机器学习]
tags: [机器学习]
comments: true
---
# 目录：

* Kramdown table of contents
{:toc .toc}

# 一、单一神经元网络

![]({{ site.url }}/assets/ML/OLS_BP_network_01.jpg){:height="40%" width="40%"}

单一神经元网络是最简单最基础的神经网络，上面这个图用公式表示出来就是：

$$Y = a(Z) = a(Xw) = a(w_0 + w_1 x_1 + \cdots + w_d x_d)$$

在神经网络中，$$a(·)$$一般称为激活函数。

在单一神经元网络里，选择线性的激活函数就相当于我们前文中的线性回归，选择sigmoid激活函数，就相当于前文中的逻辑回归。

我们以神经网络的形式重新编写线性回归和逻辑回归的代码，以获取对神经网络的初步认识：

## （一）线性回归-BP神经网络

（1）激活函数

很显然，线性回归里$$a(X)=X$$，所以：

~~~ python 
def ols_activation(X):
	return X
~~~

（2）向前传播

向前传播就是从输入X到输出Y的计算过程

~~~ python 
def ols_forward(X, w):
	Z = X*w
	a = ols_activation(Z)
	return a
~~~

其实这里输出就是X*w，之所以多加一个activation也是为了统一整个计算过程。

得到输出值以后，要计算输出值和真实值之间的差距，从而得到使差距最小的参数，在线性模型当中，我们使用梯度下降法，这里也是梯度下降法，只是后续会涉及到多层网络，计算过程更为复杂，统一称为**反向传播**，也就是BP传播

（3）反向传播（BP）

因为这里只有一层网络，所以和线性模型的梯度下降基本上一致，我们采取神经网络的写法：

同样是先计算梯度值：

~~~ python 
def OLS_dev(X, Y, w):
	return 1.0/n*X.T*(X*w-Y)
~~~

计算反向传播（梯度下降）过程：

~~~ python 
def OLS_BP(X, Y, w, lr, epoches):
	for i in range(epoches):
		w -= lr * OLS_dev(X, Y, w)
	return w
~~~

为了便于理解，这里梯度下降写的比前面简易一些

可以看到这里名称也发生了一些变换，之前的num_iters换成了epoches，eta换成了lr（learning rate）

（4）整合

整合起来就是：

~~~ python
class OLS_single_network():
    def __init__(self, epoches, lr):
        self.epoches = epoches
        self.lr = lr
        
    def ols_activation(self, X):
        return X
    
    
    def ols_forward(self, X, w):
        Z = X*w
        a = self.ols_activation(Z)
        return a
    
    def ols_dev(self, X, Y, w, input_num):
        return 1.0/input_num*X.T*(X*w-Y)
    
    def ols_BP(self, X, Y):
        input_num = X.shape[0]
        hidden_num = X.shape[1]
        output_num = 1
        w = np.mat(np.random.random((hidden_num, output_num)))
        for i in range(self.epoches):
            w -= self.lr * self.ols_dev(X, Y, w, input_num)
        
        return w
            
    def fit(self, X, Y):
        w = self.ols_BP(X, Y)
        y_predict = self.ols_forward(X, w)
        input_num = X.shape[0]
        print("train accuracy:{}%".format(100-1./input_num*np.sqrt(np.sum(np.square(y_predict-Y)))*100))
        return y_predict
~~~

运行
~~~ python
from sklearn.datasets import make_regression
import numpy as np
import matplotlib.pyplot as plt

X, Y = make_regression(n_samples=1000, n_features=1, noise=5) 
print(X.shape, Y.shape)
# ((1000, 1), (1000,))

# 这里没有专门区分w_i和w_0，因为没有正则化
X = np.insert(X, 0, values=1, axis=1)
X = np.mat(X)
Y = np.mat(Y).T
print(X.shape, Y.shape)
# ((1000, 2), (1000, 1))

lr = OLS_single_network(epoches=1000, lr=0.1)
y_predict = lr.fit(X, Y)
# train accuracy:83.6044609236%

xx = X[:,1].tolist()
yy = Y.tolist()
plt.scatter(xx, yy)
plt.plot(xx, y_predict.tolist(), c='r')
plt.show()
~~~

![]({{ site.url }}/assets/ML/OLS_BP_network_02.jpg){:height="40%" width="40%"}




反向传播算法：

　　三层神经网络：Layer L1是输入层，Layer L2是隐含层，Layer L3是隐含层，

手里有一堆数据{x1,x2,x3,...,xn},输出也是一堆数据{y1,y2,y3,...,yn},

现在要他们在隐含层做某种变换，让你把数据灌进去后得到你期望的输出。

如果你希望你的输出和原始输入一样，那么就是最常见的自编码模型（Auto-Encoder）。

如果你的输出和原始输入不一样，那么就是很常见的人工神经网络了，

相当于让原始数据通过一个映射来得到我们想要的输出数据，也就是我们今天要讲的话题。








参考文献：

1. deep learning


~~~ python
# -*- coding: utf-8 -*-
'''
Created on

@author: Belle
'''
from numpy.random.mtrand import randint
import numpy as np


'''双曲函数'''
def tanh(value):
    return (1 / (1 + np.math.e ** (-value)))

'''双曲函数的导数'''
def tanhDer(value):
    tanhValue = tanh(value)
    return tanhValue * (1 - tanhValue)

'''
Bp神经网络model
'''
class BpNeuralNetWorkModel:
    def __init__(self, trainningSet, label, layerOfNumber, studyRate):
        '''学习率'''
        self.studyRate = studyRate
        '''计算隐藏层神经元的数量'''
        self.hiddenNeuronNum = int(np.sqrt(trainningSet.shape[1] + label.shape[1]) + randint(1, 10))
        '''层数据'''
        self.layers = []
        '''创建输出层'''
        currentLayer = Layer()
        currentLayer.initW(trainningSet.shape[1], self.hiddenNeuronNum)
        self.layers.append(currentLayer)
        
        '''创建隐藏层'''
        for index in range(layerOfNumber - 1):
            currentLayer = Layer()
            self.layers.append(currentLayer)
            '''输出层后面不需要求权重值'''
            if index == layerOfNumber - 2:
                break
            nextLayNum = 0
            
            '''初始化各个层的权重置'''
            if index == layerOfNumber - 3:
                '''隐藏层到输出层'''
                nextLayNum = label.shape[1]
            else:
                '''隐藏层到隐藏层'''
                nextLayNum = self.hiddenNeuronNum
            currentLayer.initW(self.hiddenNeuronNum, nextLayNum)
        '''输出层的分类值'''
        currentLayer = self.layers[len(self.layers) - 1]
        currentLayer.label = label
    
    '''神经网络前向传播'''
    def forward(self, trainningSet):
        '''计算输入层的输出值'''
        currentLayer = self.layers[0]
        currentLayer.alphas = trainningSet
        currentLayer.caculateOutPutValues()
        
        preLayer = currentLayer
        for index in range(1, len(self.layers)):
            currentLayer = self.layers[index]
            '''上一层的out put values就是这一层的zValues'''
            currentLayer.zValues = preLayer.outPutValues
            '''计算alphas'''
            currentLayer.caculateAlphas()
            '''最后一层不需要求输出值，只要求出alpha'''
            if index == len(self.layers) - 1:
                break
            '''输入层计算out puts'''
            currentLayer.caculateOutPutValues()
            '''指向上一层的layer'''
            preLayer = currentLayer
    
    '''神经网络后向传播'''
    def backPropogation(self):
        layerCount = len(self.layers)
        
        '''输出层的残差值'''
        currentLayer = self.layers[layerCount - 1]
        currentLayer.caculateOutPutLayerError()
        
        '''输出层到隐藏层'''
        preLayer = currentLayer
        layerCount = layerCount - 1
        while layerCount >= 1:
            '''当前层'''
            currentLayer = self.layers[layerCount - 1]
            '''更新权重'''
            currentLayer.updateWeight(preLayer.errors, self.studyRate)
            if layerCount != 1:
                currentLayer.culateLayerError(preLayer.errors)
            layerCount = layerCount - 1
            preLayer = currentLayer
            
'''
创建层
'''
class Layer:
    def __init__(self):
        self.b = 0
    
    '''使用正态分布的随机值初始化w的值'''
    def initW(self, numOfAlpha, nextLayNumOfAlpha):
        self.w = np.mat(np.random.randn(nextLayNumOfAlpha, numOfAlpha))
    
    '''计算当前层的alphas'''
    def caculateAlphas(self):
        '''alpha = f(z)'''
        self.alphas = np.mat([tanh(self.zValues[row1,0]) for row1 in range(len(self.zValues))])
        '''求f'(z)的值（即f的导数值）'''
        self.zDerValues = np.mat([tanhDer(self.zValues[row1,0]) for row1 in range(len(self.zValues))])
    
    '''计算out puts'''
    def caculateOutPutValues(self):
        '''计算当前层z = w * alpha的的下一层的输入值'''
        self.outPutValues = self.w * self.alphas.T + self.b
    
    '''计算输出层的残差'''
    def caculateOutPutLayerError(self):
        self.errors = np.multiply(-(self.label - self.alphas), self.zDerValues)
        print("out put layer alphas ..." + str(self.alphas))
    
    '''计算其它层的残差'''
    def culateLayerError(self, preErrors):
        self.errors = np.mat([(self.w[:,column].T * preErrors.T * self.zDerValues[:,column])[0,0] for column in range(self.w.shape[1])])
    
    '''更新权重'''
    def updateWeight(self, preErrors, studyRate):
        data = np.zeros((preErrors.shape[1], self.alphas.shape[1]))
        for index in range(preErrors.shape[1]):
            data[index,:] = self.alphas * (preErrors[:,index][0,0])
        self.w = self.w - studyRate * data

'''
训练神经网络模型
@param train_set: 训练样本
@param labelOfNumbers: 训练总类别
@param layerOfNumber:  神经网络层数，包括输出层，隐藏层和输出层(默认只有一个输入层，隐藏层和输出层)
'''
def train(train_set, label, layerOfNumber = 3, sampleTrainningTime = 5000, studyRate = 0.6):
    neuralNetWork = BpNeuralNetWorkModel(train_set, label, layerOfNumber, studyRate)
    '''训练数据'''
    for row in range(train_set.shape[0]):
        '''当个样本使用梯度下降的方法训练sampleTrainningTime次'''
        for time in range(sampleTrainningTime):
            '''前向传播 '''
            neuralNetWork.forward(train_set[row,:])
            '''反向传播'''
            neuralNetWork.backPropogation()
            




~~~
